# Docker Compose for lyrics extraction service with GPU support
version: '3.8'

services:
  # Redis for job queue
  redis:
    image: redis:7-alpine
    container_name: lyrics-redis-gpu
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  # FastAPI service
  lyrics-api:
    build:
      context: .
      dockerfile: Dockerfile.lyrics
    container_name: lyrics-api-gpu
    ports:
      - "8000:8000"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LOG_LEVEL=INFO
      - LYRICS_DEVICE=cpu
      - LYRICS_COMPUTE_TYPE=int8
      - LYRICS_WHISPER_MODEL=large-v3
      - LYRICS_ENABLE_SEPARATION=true
      - LYRICS_MAX_UPLOAD_SIZE_MB=50
    volumes:
      - ./app/lyrics_service:/app/app/lyrics_service
      - lyrics_temp:/tmp/lyrics_extraction
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped

  # RQ Worker (GPU)
  lyrics-worker-gpu:
    build:
      context: .
      dockerfile: Dockerfile.lyrics
    container_name: lyrics-worker-gpu
    command: python -m app.lyrics_service.worker
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LOG_LEVEL=INFO
      - LYRICS_DEVICE=cuda
      - LYRICS_COMPUTE_TYPE=float16
      - LYRICS_WHISPER_MODEL=large-v3
      - LYRICS_ENABLE_SEPARATION=true
      - LYRICS_PREPROCESS_AUDIO=true
    volumes:
      - ./app/lyrics_service:/app/app/lyrics_service
      - lyrics_temp:/tmp/lyrics_extraction
      - whisper_cache:/root/.cache/whisper
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  redis_data:
  lyrics_temp:
  whisper_cache:
